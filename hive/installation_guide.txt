1) Download e extract
	http://ftp.unicamp.br/pub/apache/hive/hive-3.1.2/

2) Configurar variáveis de ambiente no .bashrc
	#HIVE ENVIRONMENT
	export HIVE_HOME="/home/prbpedro/Development/apps/apache-hive-3.1.2-bin"
	export PATH=$PATH:$HIVE_HOME/bin

3) Configurar arquivos nas pastas bin e conf

4) Criar diretórios do hive no hdfs
	hdfs dfs -mkdir /tmp
	hdfs dfs -chmod g+w /tmp
	hdfs dfs -mkdir -p /user/hive/warehouse
	hdfs dfs -chmod g+w /user/hive/warehouse

5) Atualizar biblioteca guava do hive com a do hadoop
	rm $HIVE_HOME/lib/guava-19.0.jar
	cp $HADOOP_HOME/share/hadoop/hdfs/lib/guava-27.0-jre.jar $HIVE_HOME/lib/

6) Remover jar s4jl do hive
	rm /home/prbpedro/Development/apps/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar

7) Fazer download e descompactar derby 
	http://mirror.nbtelecom.com.br/apache/db/derby/db-derby-10.14.2.0/

8) Configurar variáveis de ambiente do derby no .bashrc
	#DERBY ENVIRONMENT
	export DERBY_HOME=/home/prbpedro/Development/apps/db-derby-10.14.2.0-bin
	export PATH=$PATH:$DERBY_HOME/bin
	export CLASSPATH=$CLASSPATH:$DERBY_HOME/lib/derbyclient.jar

9) Copiar jars para lib hadoop e hive
	/home/prbpedro/Development/apps/db-derby-10.14.2.0-bin/lib
		derbyclient.jar
		derbytools.jar
	/home/prbpedro/Development/apps/hadoop-3.3.0/lib
	/home/prbpedro/Development/apps/apache-hive-3.1.2-bin/lib

11) Parar hadoop 
	stop-all.sh

12) Iniciar a base de dados derby do hive
	schematool -dbType derby -initSchema
	startNetworkServer &
	stopNetworkServer &

13) Iniciar hadoop
	start-all.sh

14) Executar o hive
	hive
	hive --service hiveserver2

15) Verificação do metastore
	ij
	ij version 10.14
	ij> connect 'jdbc:derby://localhost:1527/metastore_db;create=true';
	ij> show tables; 


______________________________ HCATALOG _____________________________________________________

1) Configurar variáveis de ambiente no .bashrc
	#HCATALOG ENVIRONMENT
	export HCAT_HOME=/home/prbpedro/Development/apps/apache-hive-3.1.2-bin/hcatalog
	export HCATJAR=$HCAT_HOME/share/hcatalog/hive-hcatalog-core-3.1.2.jar
	export HCATPIGJAR=$HCAT_HOME/share/hcatalog/hive-hcatalog-pig-adapter-3.1.2.jar
	export HADOOP_CLASSPATH=$HCATJAR:$HCATPIGJAR:$HIVE_HOME/lib/hive-common-3.1.2.jar:$HIVE_HOME/lib/hive-exec-3.1.2.jar:$HIVE_HOME/lib/hive-metastore-3.1.2.jar:$HIVE_HOME/lib/jdo-api-3.0.1.jar:$HIVE_HOME/lib/libfb303-0.9.3.jar:$HIVE_HOME/lib/libthrift-0.9.3.jar:$HADOOP_HOME/share/hadoop/common/lib/slf4j-api-1.7.25.jar:$HIVE_HOME/conf:$HIVE_HOME/lib/datanucleus-core-4.1.17.jar:$HIVE_HOME/lib/datanucleus-rdbms-4.1.19.jar:$HIVE_HOME/lib/datanucleus-api-jdo-4.2.4.jar:$HIVE_HOME/lib/jdo-api-3.0.1.jar:$HIVE_HOME/lib/javax.jdo-3.2.0-m3.jar:$DERBY_HOME/lib/derbyclient.jar:$HIVE_HOME/lib/antlr-runtime-3.5.2.jar:$HADOOP_HOME/etc/hadoop
	LIBJARS=`echo $HADOOP_CLASSPATH | sed -e 's/:/,/g'`
	export LIBJARS=$LIBJARS,$HIVE_HOME/lib/antlr-runtime-3.5.2.jar

2) Para rodar os jobs de MapReduce é necessário gerar um JAR com as seguintes dependências do maven

	<dependency>
		<groupId>org.apache.hive.hcatalog</groupId>
		<artifactId>hive-hcatalog-core</artifactId>
		<version>3.1.2</version>
	</dependency>

	<dependency>
		<groupId>org.apache.hive</groupId>
		<artifactId>hive-serde</artifactId>
		<version>3.1.2</version>
	</dependency>
	<dependency>
		<groupId>org.datanucleus</groupId>
		<artifactId>datanucleus-core</artifactId>
		<version>4.1.17</version>
	</dependency>
	<dependency>
		<groupId>org.datanucleus</groupId>
		<artifactId>datanucleus-rdbms</artifactId>
		<version>4.1.19</version>
	</dependency>
	<dependency>
		<groupId>org.datanucleus</groupId>
		<artifactId>datanucleus-api-jdo</artifactId>
		<version>4.2.4</version>
	</dependency>
	<dependency>
		<groupId>org.datanucleus</groupId>
		<artifactId>javax.jdo</artifactId>
		<version>3.2.0-m3</version>
	</dependency>
	<dependency>
		<groupId>javax.jdo</groupId>
		<artifactId>jdo-api</artifactId>
		<version>3.0.1</version>
	</dependency>

	Pluguin de geração do JAR:
	<build>
		<plugins>
			<plugin>
				<artifactId>maven-assembly-plugin</artifactId>
				<configuration>
					<archive>
						<manifest>
							<mainClass>hive.hcatalog.HCatalogMapReduceHiveExemple</mainClass>
						</manifest>
					</archive>
					<descriptorRefs>
						<descriptorRef>jar-with-dependencies</descriptorRef>
					</descriptorRefs>
				</configuration>
			</plugin>
		</plugins>
	</build>

3) Alterar o valor da variável 
	export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$mrjj
	# Aonde mrjj é o Jar gerado

4) Executar Job no Hadoop
	hadoop jar $mrjj hive.hcatalog.HCatalogMapReduceHiveExemple



Ref: https://phoenixnap.com/kb/install-hive-on-ubuntu
